{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc607f6",
   "metadata": {},
   "source": [
    "### Modis REALTIME Download and Processing\n",
    "Script designed to process Modis images and save them one by one.\n",
    "\n",
    "1. Find vtiles/htiles/dates for each data point\n",
    "2. define windows around them, cut image\n",
    "3. save images \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a896f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geojson as gsn\n",
    "from pyproj import Proj\n",
    "from osgeo import gdal\n",
    "from osgeo import gdalconst\n",
    "\n",
    "import tempfile\n",
    "import wget\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from azure.storage.blob import ContainerClient\n",
    "\n",
    "modis_account_name = 'modissa'\n",
    "modis_container_name = 'modis-006'\n",
    "modis_account_url = 'https://' + modis_account_name + '.blob.core.windows.net/'\n",
    "modis_blob_root = modis_account_url + modis_container_name + '/'\n",
    "\n",
    "# This file is provided by NASA; it indicates the lat/lon extents of each\n",
    "# NOTE: this was from tutorial, not actually helpful because unprojected?\n",
    "\n",
    "modis_tile_extents_url = modis_blob_root + 'sn_bound_10deg.txt'\n",
    "\n",
    "temp_dir = os.path.join(tempfile.gettempdir(),'modis_snow')\n",
    "os.makedirs(temp_dir,exist_ok=True)\n",
    "fn = os.path.join(temp_dir,modis_tile_extents_url.split('/')[-1])\n",
    "# wget.download(modis_tile_extents_url, fn)\n",
    "\n",
    "\n",
    "modis_container_client = ContainerClient(account_url=modis_account_url, \n",
    "                                         container_name=modis_container_name,\n",
    "                                                  credential=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091bb4c5",
   "metadata": {},
   "source": [
    "#### Modis/azure helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef66646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lon_to_modis_tile(lat,lon):\n",
    "    '''converts lat lon to modis tiles but reconstructing grid and its projection'''\n",
    "    \n",
    "    CELLS = 2400\n",
    "    VERTICAL_TILES = 18\n",
    "    HORIZONTAL_TILES = 36\n",
    "    EARTH_RADIUS = 6371007.181\n",
    "    EARTH_WIDTH = 2 * math.pi * EARTH_RADIUS\n",
    "\n",
    "    TILE_WIDTH = EARTH_WIDTH / HORIZONTAL_TILES\n",
    "    TILE_HEIGHT = TILE_WIDTH\n",
    "    CELL_SIZE = TILE_WIDTH / CELLS\n",
    "    \n",
    "    MODIS_GRID = Proj(f'+proj=sinu +R={EARTH_RADIUS} +nadgrids=@null +wktext')\n",
    "    \n",
    "    x, y = MODIS_GRID(lon, lat)\n",
    "    h = (EARTH_WIDTH * .5 + x) / TILE_WIDTH\n",
    "    v = -(EARTH_WIDTH * .25 + y - (VERTICAL_TILES - 0) * TILE_HEIGHT) / TILE_HEIGHT\n",
    "    \n",
    "    return int(h), int(v)\n",
    "\n",
    "\n",
    "def list_blobs_in_folder(container_name,folder_name):\n",
    "    \"\"\"\n",
    "    List all blobs in a virtual folder in an Azure blob container\n",
    "    \"\"\"\n",
    "    \n",
    "    files = []\n",
    "    generator = modis_container_client.list_blobs(name_starts_with=folder_name)\n",
    "    for blob in generator:\n",
    "        files.append(blob.name)\n",
    "    return files\n",
    "        \n",
    "    \n",
    "def list_hdf_blobs_in_folder(container_name,folder_name):\n",
    "    \"\"\"\"\n",
    "    List .hdf files in a folder\n",
    "    \"\"\"\n",
    "    \n",
    "    files = list_blobs_in_folder(container_name,folder_name)\n",
    "    files = [fn for fn in files if fn.endswith('.hdf')]\n",
    "    return files\n",
    "\n",
    "# daynum = '2014236'\n",
    "def daynum_gen(date_time):\n",
    "    '''converts date time objects to filename'''\n",
    "    doy = date_time.timetuple().tm_yday\n",
    "    year = date_time.year\n",
    "    return str(year) + '{:03d}'.format(doy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19b5534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_downloader(tiles, centroids, out_dataset, prod_name, verbose = False):\n",
    "    \"\"\"\"\"\"\n",
    "    cell_ids = []\n",
    "    i = 0\n",
    "    for date_tile in tqdm(tiles.keys()):\n",
    "        print(\"\\n\",i)\n",
    "\n",
    "        date = date_tile[0]\n",
    "        daynum = daynum_gen(date)\n",
    "        daynum_og = daynum #to save later\n",
    "        tile_num = (date_tile[1],date_tile[2])\n",
    "\n",
    "      \n",
    "        folder = prod_name + '/' + '{:0>2d}/{:0>2d}'.format(date_tile[1],date_tile[2]) + '/' + daynum\n",
    "\n",
    "        # Find all HDF files from this tile on this day\n",
    "        filenames = list_hdf_blobs_in_folder(modis_container_name,folder)\n",
    "        print('Found {} matching file(s):'.format(len(filenames)))\n",
    "        for fn in filenames:\n",
    "            print(fn)\n",
    "        file_root = filenames.copy()\n",
    "        \n",
    "        if len(file_root) > 1: #images may come in multiples\n",
    "            print(\"multiple files found: \", len(file_root))\n",
    "            blob_name1 = filenames[0]\n",
    "            blob_name2 = filenames[1]\n",
    "            \n",
    "            # Download to a temporary file\n",
    "            url1 = modis_blob_root + blob_name1\n",
    "            url2 = modis_blob_root + blob_name2\n",
    "\n",
    "            filename = os.path.join(temp_dir,blob_name1.replace('/','_'))\n",
    "            if not os.path.isfile(filename):\n",
    "                wget.download(url1,filename)\n",
    "                \n",
    "            filename = os.path.join(temp_dir,blob_name2.replace('/','_'))\n",
    "            if not os.path.isfile(filename):\n",
    "                wget.download(url2,filename)\n",
    "            rds1 = rxr.open_rasterio(filename)\n",
    "            rds2 = rxr.open_rasterio(filename)\n",
    "            \n",
    "            #find highest quality image\n",
    "            rds1_quality = ((rds1.NDSI_Snow_Cover_Basic_QA.values >0) | (rds1.NDSI_Snow_Cover_Basic_QA.values < 2)).sum()\n",
    "            rds2_quality = ((rds2.NDSI_Snow_Cover_Basic_QA.values >0) | (rds2.NDSI_Snow_Cover_Basic_QA.values < 2)).sum()\n",
    "            \n",
    "            rds = rds1 if rds1_quality >= rds2_quality else rds2 \n",
    "                \n",
    "        else:\n",
    "            # Work with the first returned URL\n",
    "            file_found = False\n",
    "            breaker = 1\n",
    "            while not file_found and breaker <= 5:\n",
    "                try:\n",
    "                    blob_name = filenames[0]\n",
    "                    file_found = True\n",
    "                except IndexError:\n",
    "                    print(\"No file found: tile {} date {}\".format(tile_num,daynum))\n",
    "                    date -= timedelta(days=1)\n",
    "                    daynum = daynum_gen(date) \n",
    "\n",
    "                    breaker +=1 \n",
    "                    print(\"trying:\", daynum)\n",
    "            if breaker == 5:\n",
    "                raise ValueError(\"Image\", tile_num, daynum, \"not found\")\n",
    "\n",
    "\n",
    "            # Download to a temporary file\n",
    "            url = modis_blob_root + blob_name\n",
    "            filename = os.path.join(temp_dir,blob_name.replace('/','_'))\n",
    "            if not os.path.isfile(filename):\n",
    "                wget.download(url,filename)\n",
    "\n",
    "            rds = rxr.open_rasterio(filename)\n",
    "\n",
    "        #####reproject#####\n",
    "        image = rds.rio.reproject(dst_crs=\"EPSG:4326\")\n",
    "        for var in image.data_vars:\n",
    "            image[var]=image[var].astype(image[var].dtype,keep_attrs = False) \n",
    "\n",
    "\n",
    "        #####create blocks around centroids#####    \n",
    "        cells = tiles[date_tile]\n",
    "        for cell in cells:\n",
    "            center = centroids[cell]\n",
    "\n",
    "\n",
    "            x_idx = np.nanargmin(np.abs(image.x.values - center[0]))\n",
    "            y_idx = np.nanargmin(np.abs(image.y.values - center[1]))\n",
    "\n",
    "            #subset 21x21 square\n",
    "            xmin, xmin_actual, xmax = max(x_idx -10, 0) , x_idx -10, x_idx + 11 \n",
    "            ymin, ymin_actual, ymax = max(y_idx -10, 0) , y_idx -10, y_idx + 11\n",
    "\n",
    "            sub_image = image[dict(x= slice(xmin,xmax), y= slice(ymin,ymax))]\n",
    "\n",
    "            try: # in case we're against boundary\n",
    "                sub_image = sub_image.squeeze().to_array().to_numpy()\n",
    "                out_dataset[i] = sub_image\n",
    "            except ValueError as e:                \n",
    "                #flip and reflip before saving because coding's hard\n",
    "                sub_image = np.swapaxes(sub_image, 1,2)\n",
    "                \n",
    "                image_shape = tuple(image.dims[d] for d in ['x', 'y'])\n",
    "                simage_shape = sub_image.shape\n",
    "                if verbose:\n",
    "                    print(e)\n",
    "                    print(\"Out of bounds error, padding with 0 for day/grid:\", daynum_og, cell)\n",
    "\n",
    "                    print(\"input shape: \", image_shape, \"output shape\", simage_shape)\n",
    "                    print(\"max/min\", xmax, ymax, xmin, ymin)\n",
    "                    \n",
    "                #pad with necessary columns\n",
    "                if xmin_actual < 0:                    \n",
    "                    fill = np.zeros((out_dataset.shape[1],\n",
    "                                     0-xmin_actual, simage_shape[1]))\n",
    "                    sub_image = np.concatenate((fill, sub_image), axis= 1)\n",
    "                    simage_shape = sub_image.shape\n",
    "                    if verbose:\n",
    "                        print(\"off left\")\n",
    "                        print(\"updated simage_shape\", simage_shape)\n",
    "                    \n",
    "                elif xmax > image_shape[0]:\n",
    "                    fill = np.zeros((out_dataset.shape[1],\n",
    "                                    xmax- image_shape[0], simage_shape[1]))\n",
    "                    sub_image = np.concatenate((sub_image, fill), axis=1)\n",
    "                    simage_shape = sub_image.shape\n",
    "                    print(\"off right\")\n",
    "                    print(\"updated simage_shape\", simage_shape)\n",
    "                \n",
    "                if ymin_actual < 0 :\n",
    "                    fill = np.zeros((out_dataset.shape[1],\n",
    "                                   21, 0-ymin_actual ))\n",
    "                    sub_image = np.concatenate((fill, sub_image), axis=2)\n",
    "                    simage_shape = sub_image.shape\n",
    "                    if verbose:\n",
    "                        print(\"off up\")\n",
    "                        print(\"updated simage_shape\", simage_shape)\n",
    "\n",
    "                elif ymax > image_shape[1]:\n",
    "                    fill = np.zeros((out_dataset.shape[1],\n",
    "                                     21, ymax - image_shape[1] ))\n",
    "                    sub_image = np.concatenate((sub_image,fill), axis=2)\n",
    "                    simage_shape = sub_image.shape\n",
    "                    if verbose:\n",
    "                        print(\"off down\")\n",
    "                        print(\"updated simage_shape\", simage_shape)\n",
    "                    \n",
    "                sub_image = np.swapaxes(sub_image, 1,2)\n",
    "                out_dataset[i] = sub_image\n",
    "                \n",
    "                \n",
    "                \n",
    "            cell_ids.append((cell, daynum_og)) \n",
    "\n",
    "            i+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "    return cell_ids, out_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09d243",
   "metadata": {},
   "source": [
    "Ingest training + testing geodata and timestamps\n",
    "\n",
    "Note: paths are currently absolute, but happy to make them work on both machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee14526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20759\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/Matt/Documents/Python Scripts/SnowComp/dat/grid_cells_2b.geojson\"\n",
    "with open(path) as f:\n",
    "    gj = gsn.load(f)\n",
    "print(len(gj['features']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9bb7f",
   "metadata": {},
   "source": [
    "Estimate centroids for lat_lon calculations by taking mean of points (not actual centroid because of projection and great circle distance?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a705bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = {} #cellid : centroid\n",
    "\n",
    "for cell in range(len(gj['features'])):\n",
    "    assert len(gj['features'][cell]['geometry']['coordinates'][0]) == 5 #coordinates have repeat on fifth, make sure this is universal\n",
    "    \n",
    "    cell_id =gj['features'][cell]['properties']['cell_id']\n",
    "    centroid = list(np.mean(\n",
    "        gj['features'][cell]['geometry']['coordinates'][0][0:4],\n",
    "        axis = 0)) #lazy centroid calculation\n",
    "    centroids[cell_id] = centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae694cc",
   "metadata": {},
   "source": [
    "1. Ingest training, testing, submission datasets\n",
    "2. Find what tiles (time, h,v) each image are stored in\n",
    "3. store by cell_id, recall later for centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d00ab312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"C:/Users/Matt/Documents/Python Scripts/SnowComp/dat/submission_format_2b.csv\")\n",
    "\n",
    "submission.rename({\"Unnamed: 0\":\"cell_id\"}, axis=1, inplace=True)\n",
    "DATE = \"2022-02-10\"\n",
    "# submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789b5c1d",
   "metadata": {},
   "source": [
    "## Download relevant images\n",
    "\n",
    "Process and save smaller images one by one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445eb64",
   "metadata": {},
   "source": [
    "## Submission images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef7a73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a26d9921a3e4de7b1d3309226b69be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_ids = submission['cell_id']\n",
    "\n",
    "# create dictionary tiles_sub (DATE, lat, lon) : [cell_ids]\n",
    "counter_sub = 0 \n",
    "tiles_sub = defaultdict(list)\n",
    "for cell in tqdm(cell_ids):\n",
    "    modis_tile = lat_lon_to_modis_tile(centroids[cell][1], centroids[cell][0])\n",
    "    tiles_sub[(datetime.fromisoformat(DATE),) + modis_tile].append(cell)\n",
    "    counter_sub += 1\n",
    "    \n",
    "print(\"total squares:\", counter_sub)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607efecf",
   "metadata": {},
   "source": [
    "Load Terra Submission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2ba5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = 'MOD10A1'\n",
    "\n",
    "#initialize empty array\n",
    "dataset_sub_t = np.empty((counter_sub, 7, 21, 21)) #(image, band, row, column)\n",
    "\n",
    "# download dataset\n",
    "cell_ids_sub, dataset_sub_t = images_downloader(tiles_sub, centroids, dataset_sub_t, product)\n",
    "    \n",
    "#####save output#####\n",
    "# output_path = \"C:/Users/Matt/Dropbox/SnowComp/realtimeData/\"+ \"Modis_subT_\"+str(DATE)+\".npy\"\n",
    "# np.save(output_path,dataset_sub_t)\n",
    "\n",
    "path_ids = \"C:/Users/Matt/Dropbox/SnowComp/realtimeData/\"+ \"cell_snow_ids_sub\"+str(DATE)+\".pkl\"\n",
    "with open(path_ids, 'wb') as handle:\n",
    "    pickle.dump(cell_ids_sub, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4285d80",
   "metadata": {},
   "source": [
    "Load Aqua Submission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b87c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = 'MYD10A1'\n",
    "\n",
    "#initialize empty array\n",
    "dataset_sub_a = np.empty((counter_sub, 7, 21, 21)) #(image, band, row, column)\n",
    "\n",
    "# download dataset\n",
    "cell_ids_sub, dataset_sub_a = images_downloader(tiles_sub, centroids, dataset_sub_a, product)\n",
    "    \n",
    "#####save output#####\n",
    "# output_path = \"C:/Users/Matt/Dropbox/SnowComp/realtimeData/\"+ \"Modis_subA_\"+str(DATE)+\".npy\"\n",
    "# np.save(output_path,dataset_sub_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa05180",
   "metadata": {},
   "source": [
    "## Recombine images, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f161d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset = np.concatenate((dataset_sub_t[:,0:1,:,:],dataset_sub_a[:,0:1,:,:]), axis = 1)\n",
    "sub_dataset = sub_dataset/255\n",
    "\n",
    "output_path = \"C:/Users/Matt/Dropbox/SnowComp/realtimeData/\"+ \"Modis_sub_\"+str(DATE)+\".npy\"\n",
    "np.save(output_path, sub_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43459c5c",
   "metadata": {},
   "source": [
    "### Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many filled with missing or empty\n",
    "def data_quality_checker(dataset):\n",
    "    which_idx = np.all(dataset[:,0,:,:]>100, axis = (1,2))\n",
    "    bad_images = dataset[np.all(dataset[:,0,:,:]>100, axis = (1,2))] \n",
    "    print(\"all missing:\", np.sum(np.all(dataset[:,0,:,:]>100, axis = (1,2))),\n",
    "         \"of:\", dataset.shape[0])\n",
    "    \n",
    "    \n",
    "    return bad_images, which_idx\n",
    "\n",
    "def random_bad_plot(bad_images):\n",
    "    idx = random.randrange(bad_images.shape[0])\n",
    "    plt.imshow(bad_images[idx,0,:,:])\n",
    "    print(idx)\n",
    "    \n",
    "bad_images, which_idx_a = data_quality_checker(dataset_a)\n",
    "bad_images, which_idx_t = data_quality_checker(dataset_t)\n",
    "\n",
    "print(\"overlapping:\",  np.sum(which_idx_a & which_idx_t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
