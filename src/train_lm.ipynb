{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdJinEYfGAfZ"
      },
      "source": [
        "# Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-rSKk25BOni",
        "outputId": "8da83fe6-0f8e-4caf-ea15-b4556a68bf23",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOfpo3htBS9M"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX6-3_9vGDCe"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D-VlHUVdeyRw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Installs\n",
        "# Important library for many geopython libraries\n",
        "!apt install gdal-bin python-gdal python3-gdal &> /dev/null\n",
        "# Install rtree - Geopandas requirment\n",
        "!apt install python3-rtree &> /dev/null\n",
        "# Install Geopandas\n",
        "!pip install git+git://github.com/geopandas/geopandas.git &> /dev/null\n",
        "# Install descartes - Geopandas requirment\n",
        "!pip install descartes &> /dev/null\n",
        "!pip install geopandas rioxarray &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WWfdTWVtBXkw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import gdal\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from osgeo import gdal, gdalconst\n",
        "from scipy.interpolate import UnivariateSpline\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn import linear_model, metrics\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "import plotly\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "pd.options.mode.chained_assignment = None \n",
        "DATA_PATH = 'drive/MyDrive/fall21/snowcast/data/'\n",
        "WEATHER_DIR = DATA_PATH + 'weather/'\n",
        "PRED_PATH = DATA_PATH + 'predictions/'\n",
        "\n",
        "START_DATE = datetime(2012, 6, 30)\n",
        "END_DATE = datetime(2021, 6, 29)\n",
        "\n",
        "gdal.UseExceptions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alg42J5rOqJ3"
      },
      "source": [
        "# Get main data sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AkHSH6D09nRg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Read in data\n",
        "metadata = pd.read_csv(DATA_PATH + 'ground_measures_metadata.csv')\n",
        "train_inp = pd.read_csv(DATA_PATH + 'ground_measures_train_features.csv')\n",
        "test_inp = pd.read_csv(DATA_PATH + 'ground_measures_test_features.csv')\n",
        "train_labels = pd.read_csv(DATA_PATH + 'train_labels.csv')\n",
        "grid_cells = gpd.read_file(DATA_PATH + 'grid_cells.geojson')\n",
        "submission_format = gpd.read_file(DATA_PATH + 'submission_format.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZxsNj1Da8VX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qGbn6xMTLpp",
        "outputId": "dc457d57-1119-4fb9-e819-6e6c0dcb53d3",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
            "\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "#@title Get metadata for grid cells\n",
        "cell_metadata = grid_cells\n",
        "cell_metadata['centroid'] = cell_metadata['geometry'].centroid\n",
        "cell_metadata['longitude'] = cell_metadata['centroid'].x\n",
        "cell_metadata['latitude'] = cell_metadata['centroid'].y\n",
        "cell_metadata = cell_metadata[['cell_id', 'longitude', 'latitude']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CuwwWHhoTOUs",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Format dfs\n",
        "def get_rmse(df, actual='actual_snowpack', predicted='snowpack'):\n",
        "    return ((df[actual] - df[predicted]) ** 2).mean() ** 0.5\n",
        "\n",
        "def pivot_df(df, id_col, ignore_cols=None):\n",
        "    if not ignore_cols:\n",
        "        ignore_cols = []\n",
        "    date_cols = [x for x in df.columns if x not in [id_col] + ignore_cols]\n",
        "    dfs = []\n",
        "    for day in date_cols:\n",
        "        day_df = df[[id_col, day]].rename({day: 'snowpack'}, axis=1)\n",
        "        day_df['date'] = day\n",
        "        dfs.append(day_df)\n",
        "    return pd.concat(dfs)\n",
        "\n",
        "def get_day_of_season(doy):\n",
        "    return doy + 365 - 335 if doy < 335 else doy - 335\n",
        "\n",
        "def add_time_cols(df):\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df['doy'] = df['date'].dt.dayofyear\n",
        "    df['dos'] = df['doy'].apply(get_day_of_season)\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['season'] = df['year']\n",
        "    df.loc[df['doy'] < 335, 'season'] -= 1\n",
        "    return df\n",
        "\n",
        "def clean_train_test(df, id_col='station_id', metadata_df=None):\n",
        "    df = pivot_df(df, id_col)\n",
        "    if metadata_df is not None:\n",
        "        df = df.merge(metadata_df)\n",
        "    return add_time_cols(df)\n",
        "\n",
        "\n",
        "train = clean_train_test(train_inp.rename({'Unnamed: 0': 'station_id'}, axis=1),\n",
        "                         metadata_df=metadata)\n",
        "train2 = clean_train_test(train_labels, 'cell_id', cell_metadata).dropna()\n",
        "train_full = pd.concat([train2.rename({'cell_id': 'station_id'}, axis=1).assign(datatype='labels'),\n",
        "                        train.drop(['elevation_m', 'name'], axis=1).assign(datatype='ground')])\n",
        "\n",
        "test = clean_train_test(\n",
        "    test_inp.rename({'Unnamed: 0': 'station_id'}, axis=1), metadata_df=metadata)\\\n",
        "    .rename({'snowpack': 'actual_snowpack'}, axis=1).dropna()\\\n",
        "    .merge(train[['station_id', 'state']].drop_duplicates())\n",
        "\n",
        "to_predict = clean_train_test(submission_format.drop('geometry', axis=1), 'cell_id', cell_metadata)\n",
        "\n",
        "# round(test['actual_snowpack'].std(), 3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yyA8auZP-MHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_Meid6b6Dxfb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Misc helpers\n",
        "def rmse(a, p):\n",
        "    return round(((a - p) ** 2).mean() ** 0.5, 4)\n",
        "\n",
        "def quick_predict(train_df, test_df, cols, get_preds=False):\n",
        "    lm = LinearRegression().fit(train_df[cols], train_df['snowpack'])\n",
        "    train_rmse = rmse(train_df['snowpack'], lm.predict(train_df[cols]))\n",
        "    test_preds = lm.predict(test_df[cols])\n",
        "    if get_preds:\n",
        "        return train_rmse, test_preds\n",
        "    else:\n",
        "        test_rmse = rmse(test_df['actual_snowpack'], test_preds)\n",
        "        corr = round(pearsonr(test_df['actual_snowpack'], test_preds)[0], 4)\n",
        "        return train_rmse, test_rmse, corr\n",
        "\n",
        "\n",
        "def interact(df, cont_cols, dummy_col):\n",
        "    keep_cols = list(set(df.columns) - set([dummy_col] + cont_cols))\n",
        "    dummies = pd.get_dummies(df, columns=[dummy_col])\n",
        "    dummy_cols = [x for x in dummies.columns if x.startswith(dummy_col)]\n",
        "    int_df = dummies[keep_cols]\n",
        "\n",
        "    for cc in cont_cols:\n",
        "        for dc in dummy_cols:\n",
        "            int_df[cc + '_' + dc] = dummies[cc] * dummies[dc]\n",
        "\n",
        "    return int_df\n",
        "\n",
        "def get_tt_cols(df):\n",
        "    df['dos_2'] = df['dos'] ** 2\n",
        "    return interact(df, ['dos', 'dos_2'], 'state')\n",
        "\n",
        "def write_formatted_preds(preds_df, outpath):\n",
        "    preds_df = preds_df[['cell_id', 'date', 'snowpack']]\n",
        "    preds_df['date'] = preds_df['date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
        "    submission = preds_df.pivot(index='cell_id', columns='date', values='snowpack')\\\n",
        "                        .reset_index()\\\n",
        "                        .sort_values('cell_id')\n",
        "\n",
        "    assert sorted(submission.columns) == sorted(submission_format.columns[:-1])\n",
        "    assert sorted(submission['cell_id']) == sorted(submission_format['cell_id'])\n",
        "\n",
        "    submission.to_csv(PRED_PATH + '%s.csv' % outpath, index=False)\n",
        "\n",
        "def format_state(orig_df):\n",
        "    df = orig_df.dropna()\n",
        "    df['dos_2'] = df['dos'] ** 2\n",
        "    return interact(df, ['dos', 'dos_2'], 'state')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08uiENUPQE2F"
      },
      "source": [
        "# Workspace"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get MODIS CNN predictions\n",
        "yvals = pd.read_csv(PRED_PATH + 'nnet/preds_0204/yvals.csv').drop('Unnamed: 0', axis=1)\n",
        "preds = np.load(PRED_PATH + 'nnet/preds_0204/preds.npy')\n",
        "\n",
        "yvals['modis_pred'] = preds\n",
        "modis_train_preds = train_full.copy()\n",
        "modis_train_preds['date_temp'] = modis_train_preds['date']\n",
        "modis_train_preds['date'] = modis_train_preds.apply(\n",
        "    lambda x: int(str(x['year']) + str(x['doy']).zfill(3)), axis=1)\n",
        "modis_train_preds = modis_train_preds.merge(yvals.rename({'cell_id': 'station_id'}, axis=1))\\\n",
        "    .drop('date', axis=1).rename({'date_temp': 'date'}, axis=1)\n",
        "\n",
        "\n",
        "modis_test_preds = test.drop(['elevation_m', 'name'], axis=1)\\\n",
        "    .rename({'actual_snowpack': 'snowpack'}, axis=1)\n",
        "modis_test_preds['date_temp'] = modis_test_preds['date']\n",
        "modis_test_preds['date'] = modis_test_preds.apply(\n",
        "    lambda x: int(str(x['year']) + str(x['doy']).zfill(3)), axis=1)\n",
        "modis_test_preds = modis_test_preds.merge(yvals.rename({'cell_id': 'station_id'}, axis=1))\\\n",
        "    .drop('date', axis=1).rename({'date_temp': 'date'}, axis=1)\n",
        "\n",
        "modis_all_preds = pd.concat([modis_train_preds, modis_test_preds])\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "11T0iqNP0SO1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get sentinel CNN predictions\n",
        "folder = 'preds_0214'\n",
        "\n",
        "preds = np.load(PRED_PATH + 'nnet/%s/sentpreds.npy' % folder)\n",
        "yvals = pd.read_csv(PRED_PATH + 'nnet/%s/sent_ymeta.csv' % folder).drop('Unnamed: 0', axis=1)\n",
        "yvals['sat_pred'] = preds\n",
        "yvals['date'] = pd.to_datetime(yvals['date'])\n",
        "\n",
        "sat_train_preds = train_full.merge(modis_train_preds[['station_id', 'date', 'modis_pred']])\n",
        "sat_train_preds = sat_train_preds.merge(yvals.rename({'cell_id': 'station_id'}, axis=1))\n",
        "\n",
        "sat_test_preds = test.drop(['elevation_m', 'name'], axis=1)\\\n",
        "    .rename({'actual_snowpack': 'snowpack'}, axis=1)\\\n",
        "    .merge(yvals.rename({'cell_id': 'station_id'}, axis=1))\\\n",
        "    .merge(modis_test_preds[['station_id', 'date', 'modis_pred']])\n",
        "sat_all_preds = pd.concat([sat_train_preds, sat_test_preds])\n",
        "\n"
      ],
      "metadata": {
        "id": "ljT-nSuGl5rG",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "77PVO9kPcS1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get MODIS submission CNN predictions\n",
        "nn_sub_preds = np.load(PRED_PATH + 'nnet/preds_0204/subpred.npy')\n",
        "\n",
        "def format_df(df):\n",
        "    df['dos_2'] = df['dos'] ** 2\n",
        "    return df\n",
        "\n",
        "\n",
        "path_id = DATA_PATH + 'modis/cell_ids_sub.pkl'\n",
        "with open(path_id, 'rb') as handle:\n",
        "    cell_ids = pickle.load(handle)\n",
        "\n",
        "modis_pred_df = format_df(to_predict)\n",
        "sub_order = pd.DataFrame({'identifier': ['-'.join(x) for x in cell_ids],\n",
        "                          'order': [x for x in range(len(cell_ids))]})\n",
        "modis_pred_df['identifier'] = modis_pred_df.apply(\n",
        "    lambda x: x['cell_id'] + '-' + str(x['year']) + str(x['doy']).zfill(3), axis=1)\n",
        "modis_pred_df = modis_pred_df.merge(sub_order).sort_values('order')\\\n",
        "    .assign(modis_pred=nn_sub_preds)\n"
      ],
      "metadata": {
        "id": "6KHZs1Zey3PW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get Sentinel submission CNN predictions\n",
        "nn_sub_preds = np.load(PRED_PATH + 'nnet/%s/sent_subpred.npy' % folder)\n",
        "sub_yvals = pd.concat([pd.read_csv(PRED_PATH + 'nnet/%s/sent_sub1_meta.csv' % folder),\n",
        "                       pd.read_csv(PRED_PATH + 'nnet/%s/sent_sub2_meta.csv' % folder)])\n",
        "sub_yvals['sat_pred'] = nn_sub_preds\n",
        "sub_yvals['date'] = pd.to_datetime(sub_yvals['date'])\n",
        "\n",
        "pred_df = format_df(to_predict).merge(sub_yvals[['cell_id', 'date', 'sat_pred']])\\\n",
        "    .merge(modis_pred_df[['cell_id', 'date', 'modis_pred']])\n",
        "\n"
      ],
      "metadata": {
        "id": "sqH9v-7cnAhs",
        "cellView": "form"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QF77N0w76IQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Write predictions\n",
        "cols = ['dos', 'dos_2', 'modis_pred', 'sat_pred']\n",
        "\n",
        "for df in [sat_train_preds, sat_test_preds, sat_all_preds]:\n",
        "    df['dos_2'] = df['dos'] ** 2\n",
        "\n",
        "lm = LinearRegression().fit(sat_all_preds[cols],\n",
        "                            sat_all_preds['snowpack'])\n",
        "pred_df['snowpack'] = lm.predict(pred_df[cols])\n",
        "\n",
        "write_formatted_preds(pred_df[['cell_id', 'snowpack', 'date']], 'preds')"
      ],
      "metadata": {
        "id": "TsJwJ5UErkP_",
        "cellView": "form"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_XqBJr0UazNj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "train_lm.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aEIx4hFBCzCV"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}