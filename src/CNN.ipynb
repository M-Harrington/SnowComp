{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9b5b40b",
      "metadata": {
        "id": "f9b5b40b"
      },
      "outputs": [],
      "source": [
        "dimport pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ebf6eca",
      "metadata": {
        "id": "0ebf6eca"
      },
      "source": [
        "### Load in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1a5162",
      "metadata": {
        "id": "9b1a5162"
      },
      "outputs": [],
      "source": [
        "input_path = \"C:/Users/Matt/Dropbox/SnowComp/ModisImages.npy\"\n",
        "dataset = np.load(input_path)\n",
        "\n",
        "path_ids = \"C:/Users/Matt/Dropbox/SnowComp/cell_ids.pkl\"\n",
        "with open(path_ids, 'rb') as handle:\n",
        "    cell_ids = pickle.load( handle)\n",
        "\n",
        "train_y = pd.read_csv(\"C:/Users/Matt/Documents/Python Scripts/SnowComp/dat/train_labels.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b947c93d",
      "metadata": {
        "id": "b947c93d"
      },
      "outputs": [],
      "source": [
        "#labels helpers and processing\n",
        "def pivot_df(df, id_col, ignore_cols=None):\n",
        "    if not ignore_cols:\n",
        "        ignore_cols = []\n",
        "    date_cols = [x for x in df.columns if x not in [id_col] + ignore_cols]\n",
        "    dfs = []\n",
        "    for day in date_cols:\n",
        "        day_df = df[[id_col, day]].rename({day: 'snowpack'}, axis=1)\n",
        "        day_df['date'] = day\n",
        "        dfs.append(day_df)\n",
        "    return pd.concat(dfs)\n",
        "\n",
        "def daynum_gen(date_time):\n",
        "    '''converts date time objects to filename'''\n",
        "    date_time = datetime.fromisoformat(date_time)\n",
        "    doy = date_time.timetuple().tm_yday\n",
        "    year = date_time.year\n",
        "    return str(year) + '{:03d}'.format(doy)\n",
        "\n",
        "train_y = pivot_df(train_y, 'cell_id').dropna()\n",
        "train_y['date']=train_y['date'].map(daynum_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edac2dc0",
      "metadata": {
        "id": "edac2dc0"
      },
      "outputs": [],
      "source": [
        "#sort train_y so it has correct order before stripping labels\n",
        "train_y['idx'] = train_y['cell_id'] +\"-\"+train_y['date']\n",
        "sorter = [iden +\"-\" +date for  iden, date  in cell_ids]\n",
        "train_y = train_y.set_index('idx')\n",
        "train_y = train_y.loc[sorter]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4197592",
      "metadata": {
        "id": "e4197592",
        "outputId": "0ecda165-b5d7-48a3-feee-f0649bf9af67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(91490, 3)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.shape\n",
        "train_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e0dc68",
      "metadata": {
        "id": "d1e0dc68",
        "outputId": "d35b3a9a-8a03-4c12-c682-43f0cf26cfa1"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6680/340166763.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "np.random.seed(111)\n",
        "tf.random.set_seed(111)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134a8f9b",
      "metadata": {
        "id": "134a8f9b",
        "outputId": "bdcbe4ab-8cf6-4d19-8366-62c3f3a22dfa"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (Temp/ipykernel_6680/3871919539.py, line 10)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Matt\\AppData\\Local\\Temp/ipykernel_6680/3871919539.py\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    model.add(Dense(1, activation=))\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "#define model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(30, kernel_size=5, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(15, kernel_size=4, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(5, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='relu'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "model.compile(loss='mean_squared_error', optimizer=\"adam\", metrics=[rmse])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0DRh-YaW05Eu"
      },
      "id": "0DRh-YaW05Eu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic pytorch CNN"
      ],
      "metadata": {
        "id": "xzy-yKA8089p"
      },
      "id": "xzy-yKA8089p"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4hSy2cPa0w1L"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4hSy2cPa0w1L"
    },
    {
      "cell_type": "code",
      "source": [
        "# Do categorical preds to start (change)\n",
        "train_y['cat'] = train_y['snowpack'].apply(lambda x: 1 if x > 15 else 0)\n",
        "dataset.shape, train_y.shape"
      ],
      "metadata": {
        "id": "Y6LVO6sr0cNn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Y6LVO6sr0cNn"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define simple CNN\n",
        "# From: https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html\n",
        "# Also used: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
        "\n",
        "START_D = 14\n",
        "START_HW = 21\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def _conv_calc(self, in_dim, pad, stride, k):\n",
        "        out = int(np.floor((in_dim + 2 * pad - (k - 1) - 1) / stride + 1))\n",
        "        return out\n",
        "\n",
        "    def __init__(self, cdim1, cdim2, kernel_sz, dropout,\n",
        "                 ldim, nclasses):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(14, cdim1, kernel_sz, 1)\n",
        "        c1_dim = self._conv_calc(START_HW, 0, 1, kernel_sz)\n",
        "        # print('c1 dim:', c1_dim)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(cdim1, cdim2, kernel_sz, 1)\n",
        "        c2_dim = self._conv_calc(c1_dim, 0, 1, kernel_sz)\n",
        "        # print('c2 dim:', c2_dim)\n",
        "        \n",
        "        self.maxpool1 = nn.MaxPool2d(2)\n",
        "        mp1_dim = self._conv_calc(c2_dim, 0, 2, 2)\n",
        "        # print('mp1 dim:', mp1_dim)\n",
        "        \n",
        "        # self.conv3 = nn.Conv2d(cdim2, cdim2, kernel_sz, 1)\n",
        "\n",
        "        self.dropout1 = nn.Dropout2d(dropout)\n",
        "        self.dropout2 = nn.Dropout2d(2 * dropout)\n",
        "\n",
        "        flattened_dim = cdim2 * mp1_dim * mp1_dim\n",
        "        print(flattened_dim)\n",
        "        self.fc1 = nn.Linear(flattened_dim, ldim)\n",
        "        # self.fc1 = nn.Linear(8192, ldim)\n",
        "        self.fc2 = nn.Linear(ldim, nclasses)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        # x = self.dropout1(x)\n",
        "        # x = self.conv3(x)\n",
        "        # x = F.relu(x)\n",
        "\n",
        "        # x = F.max_pool2d(x, 2)\n",
        "        x = self.maxpool1(x)\n",
        "        \n",
        "        # x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        output = nn.Softmax(dim=1)(x)\n",
        "        return output\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_ZFymFE0nMRh"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_ZFymFE0nMRh"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helpers to get predictions and accuracy\n",
        "def predict(cnn, x, as_numpy=False):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    cnn.eval()\n",
        "    x = torch.from_numpy(x).type(torch.FloatTensor).to(device)\n",
        "    output = cnn(x)\n",
        "    if as_numpy:\n",
        "        output = output.flatten().cpu().detach().numpy()\n",
        "    cnn.train()\n",
        "    return output\n",
        "\n",
        "def get_accuracy(cnn, x, y):\n",
        "    y = torch.from_numpy(y).type(torch.LongTensor).to(device)\n",
        "    _, outputs = torch.max(predict(cnn, x), 1)\n",
        "    loss = (outputs == y).sum()\n",
        "    return round(int(loss) / x.shape[0], 3)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vH2t7GaryngE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vH2t7GaryngE"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test run\n",
        "my_nn = Net(cdim1=8, cdim2=8, kernel_sz=3, dropout=0.25, ldim=8, nclasses=2)\n",
        "optimizer = optim.SGD(my_nn.parameters(), lr=0.001)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "test_im = torch.from_numpy(dataset[0]).reshape(1, 14, 21, 21)\n",
        "result = my_nn(test_im.type(torch.FloatTensor))\n",
        "result.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "CzIiQX4hnMOL",
        "outputId": "8aeeac04-d3f3-49a8-d991-7c26fe94f061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "id": "CzIiQX4hnMOL"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get data loaders\n",
        "train_dataset = TensorDataset(torch.Tensor(dataset),\n",
        "                              torch.Tensor(train_y['cat']).type(torch.LongTensor))\n",
        "train_loader = DataLoader(train_dataset, batch_size=64)\n",
        "\n",
        "mini_x, mini_y = np.array(dataset[:1000,]), np.array(train_y['cat'][:1000])\n",
        "mini_dataset = TensorDataset(torch.Tensor(mini_x),\n",
        "                              torch.Tensor(mini_y).type(torch.LongTensor))\n",
        "mini_loader = DataLoader(mini_dataset, batch_size=64)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j9xCBXI-nMK9"
      },
      "execution_count": null,
      "outputs": [],
      "id": "j9xCBXI-nMK9"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup net\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "my_nn.to(device)\n",
        "\n",
        "optimizer = optim.SGD(my_nn.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "writer = SummaryWriter('runs/cnn_full')\n",
        "write_index = 0\n",
        "\n",
        "N_EPOCHS = 2\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GwEGyBD2nMIR"
      },
      "execution_count": null,
      "outputs": [],
      "id": "GwEGyBD2nMIR"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f8lCAFBjzK0o"
      },
      "execution_count": null,
      "outputs": [],
      "id": "f8lCAFBjzK0o"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run net\n",
        "for epoch in range(N_EPOCHS):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(mini_loader, 0):\n",
        "        optimizer.zero_grad()\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        outputs = my_nn(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 100 == 0:\n",
        "            writer.add_scalar('Loss/train', running_loss / i, write_index)\n",
        "        write_index += 1\n",
        "\n",
        "    val_acc = get_accuracy(my_nn, mini_x, mini_y)\n",
        "    writer.add_scalar('Acc/val', val_acc, write_index)\n",
        "    print(epoch, '/', N_EPOCHS, 'complete', val_acc, running_loss / i)\n",
        "\n",
        "writer.close()\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "rZKIBpS5nMAZ",
        "outputId": "40e82096-4e7c-42b9-9b24-2e38d7fb1bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 2 complete 0.795 0.6782322029272715\n",
            "1 / 2 complete 0.796 0.6288726329803467\n",
            "Finished Training\n"
          ]
        }
      ],
      "id": "rZKIBpS5nMAZ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "CNN.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}