{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cedb618d",
   "metadata": {},
   "source": [
    "# CNN for Sentinel Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0c6824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "\n",
    "tqdm.pandas() \n",
    "\n",
    "rng = np.random.default_rng(342834)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a328645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels helpers and processing\n",
    "def pivot_df(df, id_col, ignore_cols=None):\n",
    "    if not ignore_cols:\n",
    "        ignore_cols = []\n",
    "    date_cols = [x for x in df.columns if x not in [id_col] + ignore_cols]\n",
    "    dfs = []\n",
    "    for day in date_cols:\n",
    "        day_df = df[[id_col, day]].rename({day: 'snowpack'}, axis=1)\n",
    "        day_df['date'] = day\n",
    "        dfs.append(day_df)\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "def daynum_gen(date_time):\n",
    "    '''converts date time objects to filename'''\n",
    "    date_time = datetime.fromisoformat(date_time)\n",
    "    doy = date_time.timetuple().tm_yday\n",
    "    year = date_time.year\n",
    "    return str(year) + '{:03d}'.format(doy)\n",
    "\n",
    "# Get ordered elevation training data\n",
    "def add_elevation(order, modis):\n",
    "    order = pd.DataFrame({'modis_idx': order, 'order': [x for x in range(len(order))]})\n",
    "    order['station_id'] = order['modis_idx'].apply(lambda x: '-'.join(x.split('-')[:-1]))\n",
    "    order = order.merge(elev_order).sort_values('order')\n",
    "    ordered_elev = elevation[order['DEM_order'].to_list(), :, :]\n",
    "    dim = ordered_elev.shape\n",
    "\n",
    "    return np.concatenate([modis, ordered_elev.reshape(dim[0], 1, dim[1], dim[2])], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "643fa247",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentinel Helpers\n",
    "def y_merger(x, y):\n",
    "    '''reattach y labels to sentinel'''\n",
    "    y = y.rename(columns={\"Unnamed: 0\":\"cell_id\"})\n",
    "    y = pivot_df(y, 'cell_id').dropna()\n",
    "    y['date']=y['date'].map(daynum_gen)\n",
    "\n",
    "    y['idx'] = y['cell_id'] + \"-\" + y['date']\n",
    "    y = y.set_index('idx')\n",
    "\n",
    "    x['idx'] = x['cell_id'] +\\\n",
    "         \"-\" +x['date_long'].astype(str)\n",
    "    x = x.set_index('idx')\n",
    "\n",
    "    return x.join(y['snowpack'])\n",
    "\n",
    "#preprocessing helpers\n",
    "def masker(x,y):\n",
    "    mask = np.all(x > -99, axis = (1,2))\n",
    "    print(mask.sum(), \"of\", len(mask))\n",
    "    \n",
    "    return x[mask], y[mask]\n",
    "\n",
    "def minmaxscaler(x):\n",
    "    print(\"min\", round(x.min(),3), \"max\", round(x.max(),3))\n",
    "    x = (x - x.min())/(x.max() - x.min())\n",
    "                   \n",
    "    return x\n",
    "\n",
    "def reshaper(ds):\n",
    "    #readjust dimensions\n",
    "    dim0 = ds.shape[0]\n",
    "    dim1 = ds.shape[1]\n",
    "    dim2 = ds.shape[2]\n",
    "\n",
    "    return ds.reshape((dim0, 1, dim1, dim2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7027d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_path = \"C:/Users/Matt/Documents/Python Scripts/SnowComp/dat/\"\n",
    "\n",
    "train_feat = pd.read_csv(y_path + \"ground_measures_train_features.csv\")\n",
    "test =pd.read_csv(y_path + \"ground_measures_test_features.csv\")\n",
    "submission = pd.read_csv(y_path + \"submission_format.csv\")\n",
    "train_y = pd.read_csv(y_path + \"train_labels.csv\") \n",
    "metadata = pd.read_csv(y_path + \"ground_measures_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94272b66",
   "metadata": {},
   "source": [
    "## Sentinel Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b8dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_path = \"C:/Users/Matt/Dropbox/SnowComp/SentinelHelper/\"\n",
    "\n",
    "sentinel_trainfeat = np.load(sent_path + \"sent_pp_trainfeat.npy\")\n",
    "sentinel_testfeat = np.load(sent_path + \"sent_pp_testfeat.npy\")\n",
    "sentinel_ylabs = np.load(sent_path + \"sent_pp_ylabs.npy\")\n",
    "\n",
    "trainfeat_meta = pd.read_csv(sent_path + \"sent_trainfeat_meta.csv\")\n",
    "testfeat_meta = pd.read_csv(sent_path + \"sent_testfeat_meta.csv\")\n",
    "ylabs_meta = pd.read_csv(sent_path + \"sent_ylabs_meta.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65cfe5",
   "metadata": {},
   "source": [
    "### Merge back in y labels, mask NAs\n",
    "\n",
    "CHECK order not adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51292f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeat_meta = y_merger(trainfeat_meta, train_feat)\n",
    "testfeat_meta = y_merger(testfeat_meta, test)\n",
    "ylabs_meta = y_merger(ylabs_meta, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0262d536",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76159 of 76410\n",
      "105571 of 106760\n",
      "38618 of 38628\n",
      "min -57.906 max 18.57\n",
      "min -50.229 max 19.536\n",
      "min -27.488 max 14.445\n"
     ]
    }
   ],
   "source": [
    "sentinel_ylabs, ylabs_meta = masker(sentinel_ylabs, ylabs_meta)\n",
    "sentinel_trainfeat, trainfeat_meta = masker(sentinel_trainfeat, trainfeat_meta)\n",
    "sentinel_testfeat, testfeat_meta = masker(sentinel_testfeat, testfeat_meta)\n",
    "\n",
    "sentinel_ylabs = minmaxscaler(sentinel_ylabs)\n",
    "sentinel_trainfeat = minmaxscaler(sentinel_trainfeat)\n",
    "sentinel_testfeat = minmaxscaler(sentinel_testfeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f976dd",
   "metadata": {},
   "source": [
    "## Define Training and Testing Sets\n",
    "\n",
    "\n",
    "`dataset -> sentinel_ylabs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f107fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb45d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64679, 41, 41)\n",
      "(11480, 41, 41)\n",
      "(11480,)\n",
      "(64679,)\n"
     ]
    }
   ],
   "source": [
    "#holdout \n",
    "mask = rng.random(len(sentinel_ylabs)) < 0.85\n",
    "\n",
    "test_grid = sentinel_ylabs[~mask]\n",
    "sentinel_ylabs = sentinel_ylabs[mask]\n",
    "\n",
    "test_grid_y = ylabs_meta['snowpack'].values[~mask]\n",
    "train_y = ylabs_meta['snowpack'].values[mask]\n",
    "\n",
    "print(sentinel_ylabs.shape)\n",
    "print(test_grid.shape)\n",
    "print(test_grid_y.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "224080c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "train_dataset = np.concatenate((sentinel_ylabs,\n",
    "                   sentinel_trainfeat), axis = 0)\n",
    "train_y = np.concatenate((train_y, trainfeat_meta['snowpack'].values),\n",
    "                    axis = 0)\n",
    "\n",
    "#shuffle\n",
    "p =rng.permutation(len(train_dataset))\n",
    "train_y = train_y[p]\n",
    "train_dataset = train_dataset[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eca8af",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f7ca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data\n",
    "dataset_test = np.concatenate((sentinel_testfeat, test_grid),\n",
    "                              axis = 0) \n",
    "\n",
    "y_test = np.concatenate((testfeat_meta['snowpack'].values, test_grid_y),\n",
    "                    axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ef2b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle\n",
    "p =rng.permutation(len(dataset_test))\n",
    "y_test = y_test[p]\n",
    "dataset_test = dataset_test[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76dc711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = reshaper(dataset_test)\n",
    "train_dataset = reshaper(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f230fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title split training and testing\n",
    "training_data = train_dataset\n",
    "testing_data = dataset_test\n",
    "train_rows = len(training_data)\n",
    "test_rows = len(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca25274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Get data loaders\n",
    "test_x, test_y = dataset_test, y_test\n",
    "test_x, test_y = torch.Tensor(test_x), torch.Tensor(test_y)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 256)\n",
    "\n",
    "mini_x, mini_y = train_dataset, train_y\n",
    "mini_x, mini_y = torch.Tensor(mini_x), torch.Tensor(mini_y)\n",
    "\n",
    "mini_dataset = TensorDataset(mini_x,\n",
    "                              mini_y)\n",
    "mini_loader = DataLoader(mini_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed805524",
   "metadata": {},
   "source": [
    "## Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "867e4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define simple CNN\n",
    "# From: https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html\n",
    "# Also used: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "START_HW = 41\n",
    "START_D = 1\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def _conv_calc(self, in_dim, pad, stride, k):\n",
    "        out = int(np.floor((in_dim + 2 * pad - (k - 1) - 1) / stride + 1))\n",
    "        return out\n",
    "\n",
    "    def __init__(self, cdim1, cdim2,cdim3, kernel_sz, dropout,\n",
    "                 ldim, print_dim = True):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #first layer\n",
    "        self.conv1 = nn.Conv2d(START_D, cdim1, kernel_sz, 1)\n",
    "        self.avgpool = nn.AvgPool2d(3, stride= 1)\n",
    "        \n",
    "        c1_dim = self._conv_calc(START_HW, 0, 1, kernel_sz)\n",
    "        mp0_dim = self._conv_calc(c1_dim, 0, 1, 3)\n",
    "        \n",
    "        #second layer\n",
    "        self.conv2 = nn.Conv2d(cdim1, cdim2, kernel_sz, 1)\n",
    "        c2_dim = self._conv_calc(mp0_dim, 0, 1, kernel_sz)\n",
    "        mp1_dim = self._conv_calc(c2_dim, 0, 1, 3)\n",
    "        \n",
    "        \n",
    "        #third layer\n",
    "        self.conv3 = nn.Conv2d(cdim2, cdim3, kernel_sz, 1)\n",
    "        c3_dim = self._conv_calc(mp1_dim, 0, 1, kernel_sz)\n",
    "        mp2_dim = self._conv_calc(c3_dim, 0, 1, 3)\n",
    "        \n",
    "        #fourth layer\n",
    "        flattened_dim = cdim3 * mp2_dim * mp2_dim\n",
    "        self.fc1 = nn.Linear(flattened_dim, ldim)\n",
    "        self.fc2 = nn.Linear(ldim, 1)\n",
    "\n",
    "        #extras\n",
    "        self.dropout1 = nn.Dropout2d(dropout)\n",
    "        self.dropout2 = nn.Dropout2d(dropout*2)\n",
    "        self.BatchNorm1 = nn.BatchNorm2d(cdim1)\n",
    "        self.BatchNorm3 = nn.BatchNorm2d(cdim3)\n",
    "        \n",
    "        if print_dim:\n",
    "            print('c1 dim:', c1_dim)\n",
    "            print('mp0 dim:', mp0_dim)\n",
    "            print('c2 dim:', c2_dim)\n",
    "            print('mp1 dim:', mp1_dim)\n",
    "            print('c3 dim:', c3_dim)\n",
    "            print('mp2 dim:', mp2_dim)\n",
    "            print('flattened_dim',flattened_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #first layer\n",
    "        x = self.conv1(x)\n",
    "#         x = torch.tanh(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.BatchNorm1(x)\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        #second layer\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "#         x = torch.tanh(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        #third layer\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.BatchNorm3(x)\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        #fourth layer\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        output = self.fc2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c5a73c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helpers to get predictions and accuracy\n",
    "def predict(cnn, x, as_numpy=False):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    cnn.eval()\n",
    "    x =x.type(torch.FloatTensor).to(device)\n",
    "    output = cnn(x)\n",
    "    if as_numpy:\n",
    "        output = output.flatten().cpu().detach().numpy() #detach removes gradients (bad)\n",
    "        \n",
    "    cnn.train()\n",
    "    return output.squeeze()\n",
    "\n",
    "def get_accuracy(cnn, x, y):\n",
    "#     y = torch.from_numpy(y).to(device)\n",
    "    outputs = predict(cnn, x,as_numpy = False)\n",
    "    \n",
    "#     print(y.shape, outputs.shape)\n",
    "    loss = ((y-outputs)**2).sum()\n",
    "    return round(loss.item(), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dd880b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 dim: 39\n",
      "mp0 dim: 37\n",
      "c2 dim: 35\n",
      "mp1 dim: 33\n",
      "c3 dim: 31\n",
      "mp2 dim: 29\n",
      "flattened_dim 12615\n"
     ]
    }
   ],
   "source": [
    "#@title Setup net\n",
    "cdim1=80; cdim2=30; cdim3 =15; kernel_sz=3; dropout=0.13; ldim=80; lrate = 0.0001\n",
    "my_nn = Net(cdim1=cdim1, cdim2=cdim2,cdim3 =cdim3, kernel_sz=kernel_sz, dropout=dropout, ldim=ldim)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "my_nn.to(device)\n",
    "optimizer = optim.Adam(my_nn.parameters(), lr=lrate)\n",
    "criterion = nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "writer = SummaryWriter('runs/sentcnn_alldata')\n",
    "write_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32459c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22369baf0e7949e28c11f70b4bcfae4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200 complete, train: 13.9288 test: 13.7836\n",
      "1 / 200 complete, train: 13.2097 test: 11.9331\n",
      "2 / 200 complete, train: 12.8615 test: 11.927\n",
      "3 / 200 complete, train: 12.5538 test: 12.1845\n",
      "4 / 200 complete, train: 12.234 test: 12.0719\n",
      "5 / 200 complete, train: 11.9658 test: 11.896\n",
      "6 / 200 complete, train: 11.7208 test: 11.747\n",
      "7 / 200 complete, train: 11.4914 test: 11.789\n",
      "8 / 200 complete, train: 11.2815 test: 11.7612\n",
      "9 / 200 complete, train: 11.085 test: 11.6754\n",
      "10 / 200 complete, train: 10.9093 test: 11.6153\n",
      "11 / 200 complete, train: 10.7281 test: 11.558\n",
      "12 / 200 complete, train: 10.5895 test: 11.7867\n",
      "13 / 200 complete, train: 10.4492 test: 11.564\n",
      "14 / 200 complete, train: 10.2987 test: 11.4742\n",
      "15 / 200 complete, train: 10.1505 test: 11.5524\n",
      "16 / 200 complete, train: 10.024 test: 11.288\n",
      "17 / 200 complete, train: 9.9115 test: 11.2293\n",
      "18 / 200 complete, train: 9.772 test: 11.1583\n",
      "19 / 200 complete, train: 9.6687 test: 11.3957\n",
      "20 / 200 complete, train: 9.5518 test: 11.3477\n",
      "21 / 200 complete, train: 9.4675 test: 11.3849\n",
      "22 / 200 complete, train: 9.3593 test: 11.2162\n",
      "23 / 200 complete, train: 9.2422 test: 11.163\n",
      "24 / 200 complete, train: 9.1561 test: 11.1386\n",
      "25 / 200 complete, train: 9.0475 test: 11.191\n",
      "26 / 200 complete, train: 8.9775 test: 11.2499\n",
      "27 / 200 complete, train: 8.8776 test: 11.2317\n",
      "28 / 200 complete, train: 8.8123 test: 11.291\n",
      "29 / 200 complete, train: 8.7182 test: 11.527\n",
      "30 / 200 complete, train: 8.6443 test: 11.3395\n",
      "31 / 200 complete, train: 8.55 test: 11.1403\n",
      "32 / 200 complete, train: 8.4883 test: 11.2079\n",
      "33 / 200 complete, train: 8.4135 test: 11.2389\n",
      "34 / 200 complete, train: 8.3368 test: 11.4564\n",
      "35 / 200 complete, train: 8.2759 test: 11.3212\n",
      "36 / 200 complete, train: 8.1821 test: 11.5183\n",
      "37 / 200 complete, train: 8.1371 test: 11.2785\n",
      "38 / 200 complete, train: 8.0655 test: 11.7029\n",
      "39 / 200 complete, train: 7.9872 test: 11.4451\n",
      "40 / 200 complete, train: 7.9427 test: 11.6287\n",
      "41 / 200 complete, train: 7.8735 test: 11.1471\n",
      "42 / 200 complete, train: 7.8044 test: 11.4684\n",
      "43 / 200 complete, train: 7.7713 test: 11.3572\n",
      "44 / 200 complete, train: 7.7223 test: 11.5756\n",
      "45 / 200 complete, train: 7.6663 test: 11.4622\n",
      "46 / 200 complete, train: 7.5935 test: 11.5091\n",
      "47 / 200 complete, train: 7.5576 test: 11.3999\n",
      "48 / 200 complete, train: 7.4864 test: 11.2031\n",
      "49 / 200 complete, train: 7.4457 test: 11.4869\n",
      "50 / 200 complete, train: 7.4032 test: 11.3565\n",
      "51 / 200 complete, train: 7.3516 test: 11.5463\n",
      "52 / 200 complete, train: 7.3015 test: 11.2384\n",
      "53 / 200 complete, train: 7.2673 test: 11.2313\n",
      "54 / 200 complete, train: 7.2176 test: 11.5369\n",
      "55 / 200 complete, train: 7.1795 test: 11.4232\n",
      "56 / 200 complete, train: 7.1392 test: 11.5483\n",
      "57 / 200 complete, train: 7.1044 test: 11.2974\n",
      "58 / 200 complete, train: 7.0583 test: 11.5787\n",
      "59 / 200 complete, train: 7.0114 test: 11.4359\n",
      "60 / 200 complete, train: 6.9707 test: 11.5622\n",
      "61 / 200 complete, train: 6.9341 test: 11.4644\n",
      "62 / 200 complete, train: 6.8865 test: 11.6192\n",
      "63 / 200 complete, train: 6.8638 test: 11.2999\n",
      "64 / 200 complete, train: 6.821 test: 11.3018\n",
      "65 / 200 complete, train: 6.7722 test: 11.4738\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 200\n",
    "\n",
    "test_loss = []\n",
    "train_loss = []\n",
    "\n",
    "#@title Run net\n",
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(mini_loader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        outputs = my_nn(inputs).squeeze()\n",
    "#         print(outputs.shape, labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         for name, param in my_nn.named_parameters():\n",
    "#             print(name, param.grad.abs().sum())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            writer.add_scalar('Loss/train', running_loss , write_index)\n",
    "        write_index += 1\n",
    "\n",
    "#     val_acc = get_accuracy(my_nn, mini_x, mini_y)\n",
    "    train_loss.append(running_loss/train_rows)\n",
    "    writer.add_scalar('Acc/val', train_loss[-1], write_index)\n",
    "    \n",
    "    #calculate test loss.\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        running_tar_loss = 0\n",
    "        for data, target in test_loader:\n",
    "            running_tar_loss += get_accuracy(my_nn, data, target.to(device))\n",
    "\n",
    "\n",
    "        test_loss.append(running_tar_loss / test_rows)\n",
    "        writer.add_scalar('Test MSE', test_loss[-1], write_index)\n",
    "\n",
    "    print(epoch, '/', N_EPOCHS,\n",
    "          'complete, train:', round(np.sqrt(train_loss[-1]), 4),\n",
    "          \"test:\", round(np.sqrt(test_loss[-1]), 4) )\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "writer.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8373efd",
   "metadata": {},
   "source": [
    "### Checking Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/Matt/Dropbox/SnowComp/RunGraphs/\" \n",
    "\n",
    "suffix = \"_\" + \\\n",
    "    str(cdim1)+ \"_\" + str(cdim2)+ \"_\" +str(cdim3)+ \"_\" + str(kernel_sz)+ \\\n",
    "    \"_\" + str(dropout)+ \"_\" + str(ldim)+ \"_\" + str(epoch) +\"_\" + str(lrate)\n",
    "\n",
    "plt.plot(range(epoch+1), np.sqrt(train_loss), label =\"train\")\n",
    "plt.plot(range(epoch+1), np.sqrt(test_loss), label =\"test\")\n",
    "plt.legend()\n",
    "plt.savefig(path +\"sent_converge_alldata\" + suffix+ \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb7f032",
   "metadata": {},
   "source": [
    "## Save Model and Predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
